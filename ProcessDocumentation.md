# Process Documentation: How we created and iterated on the service and the framework

For a detailed overview of how an Exploration works, see our [Exploration Sprint Guide](https://github.com/cds-snc/exploration-documentation/blob/main/ExplorationSprintGuide.md).

## Table of contents 
- Why Exploration? 
- How we developed it 
- What is an Exploration? 

## Why Exploration

### The problem: Partnership challenges 

One of the ways that CDS works with departments is through [Partnerships](https://digital.canada.ca/partnerships/). But in the 3 years that CDS has existed, our Partnerships have been hard and slow. 

Our delivery teams expected to roll up their sleeve and get to work from the get go, but instead ran into blockers. Constraints, such as lengthy and slow approvals, difficulty in getting access to key infrastructure, unexpected legal opinions, fuzzy problem statements and fear of launching something new, have prevented us from building *with* partners and sharing our work publicly. In some cases it’s even prevented us from taking the final step to go live. 

This lead to a difficult and stressful working environment for our delivery teams. They were often treated like vendors rather than partners. Our partnerships had a high potential to fail. And because of that, we weren’t doing a good job delivering on our mandate to change government to serve people better.

What we want is to work with partners who are open and receptive to new ways of working: Where they provide access to the people, information and infrastructure we need, and they’re comfortable with us sharing our work, methodologies and experiences beyond the project space.  

So in May of 2020, we launched our work on our new **Exploration Service** by dedicating a multi-disciplinary team to it. 

### Goals of an Exploration
It's been a long journey to get here, and the goals of the service have changed along the way, but here is where they stand now:  
- **Goal 1:** Explore the partner’s capabilities and readiness to deliver a digital service and provide practical actions to improve.
- **Goal 2:** Determine CDS’ level of involvement.
- **Goal 3:** Understand the government landscape and learn about patterns and trends.

## How we developed the service

### 1. Conducted background research

#### Secondary research 
We started out by reviewing other frameworks and models around the globe, such as maturity models and the USDS Discovery Sprint model. We thought we could possibly repurpose content from other sources, but quickly realized that a maturity model wasn't a fit for the problem we were trying to solve. Instead we used those models to map out the common themes we were seeing across the literature. The resulting thematic model became the foundation for our framework. 

Another source of secondary research, we also adapted the structure of the sprints from the [USDS Discovery Sprint](https://sprint.usds.gov/preparing/) model, which became the foundation for our service. Note: the USDS documentation linked here is a newer iteration of the one that we worked with at the beginning of our project.   

#### Primary research 
We collected primary research within CDS by talking to all of the delivery communities. We gathered their perspective on what the necessary conditions for digital service delivery were, based on their experience in government partnerships. We then mapped these needs to the thematic model that we created from our secondary research. The needs and the thematic model together became the framework.

![Diagram with rectangles. On the left side there's a rectangle labelled "Outreach", which points to "Intake", which then points to 4 different boxes. From top to bottom the boxes are labelled "Partnerships:Embedded Service Design and Delivery", "Partnerships:Exploration", Partnerships: Consulting", "Platform: Notify", "Platform:Forms". Within each of the boxes is there are more boxes showing a process through directional arrows.](https://github.com/cds-snc/exploration-documentation/blob/main/images/Assessment%20Criteria%20Synthesis.png)

#### Peer feedback sessions 
In the early stages of the process we meet with peers across other organizations including the Ontario Digital Service (ODS), Government Digital Service (GDS), and CodeforCanada(C4C) to get feedback from them.

#### Global lens
We also had global consultations from Public Digital and the Harvard Kennedy School. 

### 2. Developed a Theory of Change
We created a [Theory of Change](https://trello.com/c/IXrBSPhx/1-monitoring-and-evaluation-workshop-from-merlin-chatwin-c4c-and-laura-nelson-hamilton-ods) for the service, which informed our evaluation & monitoring plan. 

![Diagram with rectangles. On the left side there's a rectangle labelled "Outreach", which points to "Intake", which then points to 4 different boxes. From top to bottom the boxes are labelled "Partnerships:Embedded Service Design and Delivery", "Partnerships:Exploration", Partnerships: Consulting", "Platform: Notify", "Platform:Forms". Within each of the boxes is there are more boxes showing a process through directional arrows.](https://github.com/cds-snc/exploration-documentation/blob/main/images/Pre-Discovery%20Theory%20of%20Change.png)

### 3. Prototyped the framework and the service

We started by prototyping the framework criteria and questions. At the same time we also mapped the Exploration Service Model and developed processes and artifacts to support delivery of the service.

### 4. Created a CDS Intake model 
Through this project we had identified a need for a more defined and centralized Intake process, so we also took a broader look at CDS as a whole and mapped out all of our service lines. We then created a centralized Intake to serve departments coming into our pipeline.

### 5. Piloted with 4 programs, iterating between each round

Since September of 2020, we have piloted with 4 programs, in 3 departments:
- ESDC Temporary Foreign Worker Program 
- NRCan CanmetENERGY
- NRCan Flood mapping 
- IRCC Refugee Information Needs

Throughout the pilots we collect research insights on how well the process is or isn't working, and then we spend 1-2 sprints iterating in between each round.

## Naming the service: what's in a name? 

The service has evolved immensely, which is reflected in the number of times we've iterated on the name: 

**1. Departmental Assessment** 

The service began as a "vetting" tool, a "go/no go" on whether we would partner with a department. But we found that this name only described the part of the service that was evaluative. It sounded a bit like an exam and introduced a power dynamic between the org doing the assessing (us) and the one being assessed (our partners). 

**2. Pre-Discovery** 

We moved to the term "Pre-Discovery" because it sounded less intimidating and it fit into our "Discovery, Alpha, Beta, Live" model of phase patterns, which helped communicate the process. But after running a couple pilots, it became evident that the end might not always lead to a Discovery phase. We learned there's multiple ways of helping a department, and so by calling it a "Pre-Discovery" we weren't managing expectations effectively. We also found that the process had value in and of itself for a partner, and it could exist as its own service. So we decided to separate the name from the "Discovery, Alpha, Beta, Live" model.

**3. Exploration** 

The latest iteration, and where we've landed currently is "Exploration". We chose this name because we thought it was more reflective of what we're actually doing. We are Exploring digital service conditions by adapting to a new environment and providing tailored help. In other words we're meeting partners where **they're** at. Failures are okay when exploring and can bring high value and insights as we document and chart a path to build a larger view of the landscape. And even if an Exploration doesn’t lead to a Discovery, there are many useful small discoveries along the way. 

## What is an Exploration? 

We have a couple of hypotheses: 
1. If CDS better understands the existing conditions within a service context then it will be able to better tailor its help to a department, and
2. If enabling conditions and practices are present then teams will deliver better products and services. 

An Exploration is a service we deliver to:
- Understand the existing delivery conditions and practices within a service context;
- Identify ways to introduce digital practices, like conducting user research and practicing continuous improvement;
- Make recommendations on ways to shift conditions to enable those practices, if and where needed;
- Provide advice and practical actions to develop and deliver a product or service – or improve an existing one; and
- Explore how CDS might support a partner on that journey.

An Exploration is comprised of a **framework** we created and a **service** we built on top of it. The framework includes criteria organized into categories. The service is the 4-week process and all of the supporting artifacts that we use to deliver the value to the department. We separate the framework and the service into two parts because the framework could also potentially be used as a foundation for other services, such as a self-assessment service.

As we deliver the service, we’re learning and iterating on both, to improve the Exploration experience and the value that comes from participating in and running one.

### Framework: Exploring enabling conditions

To learn about a department's conditions we look at these two areas:

*To what extent do digital roles and structures exist?*
- Leadership and coherent direction (coherent policy direction, service priorities and champions)
- Enabling capacity and infrastructure (modern IT infrastructure, legal, comms and privacy guidance and support)
- Delivery capability (talent and teams with digital skills, tactics and tools)
 
*To what extent do digital practices exist?* 
- Shifting to a product delivery model
- Cultivating a culture of collaboration 
- Working in the open 
- Establishing and empowering multi-disciplinary teams 
- Conducting user research 
- Practicing continuous improvement

### Service: Where Explorations live in the CDS service model 

This is the map of our overall CDS service model. To read the boxes more closely, click on the image. 

![Diagram with rectangles. On the left side there's a rectangle labelled "Outreach", which points to "Intake", which then points to 4 different boxes. From top to bottom the boxes are labelled "Partnerships:Embedded Service Design and Delivery", "Partnerships:Exploration", Partnerships: Consulting", "Platform: Notify", "Platform:Forms". Within each of the boxes is there are more boxes showing a process through directional arrows.](https://github.com/cds-snc/exploration-documentation/blob/main/images/Business%20units%20service%20map.png)

Each of the squares represents one of the services that we offer, such as Notify, Forms, Embedded Partnerships etc, with arrows going between all of them, showing how they connect with each other and work together. 

Most services start with our new Intake process on the left side of the map. This is how departments come into the organization, and get triaged to different units depending on their needs. Intake may lead to an Exploration (the second box down on the right side), which in turn may lead to other services within CDS. 

### Service: How the process works (overview)

If you want an in-depth look at the process, check out our [How to set up and run an Exploration](https://github.com/cds-snc/exploration-documentation/blob/main/ExplorationSprintGuide.md) guide. 

This is the map of the Exploration service. To read the boxes more closely, click on the image. 

![alt text to be added](https://github.com/cds-snc/exploration-documentation/blob/main/images/Exploration_service_map.png)

Explorations are designed to take approximately 4 weeks, with a set up phase prior to that. The process goes as follows... 

Before Exploration: Set-up 
- Describe the problem space
- Map key stakeholders
- Gather background documents
- Meet with the partner to set up the sprint

Week 1: Internal onboarding 
- CDS team onboarding meeting
- Read partner documentation 
- Review and tailor sprint materials to the partner

Week 2: Gather information 
- Kick off meeting 
- Interviews
- Debriefs
- Scoring

Week 3: Synthesize and present internally 
- Follow up discussions with department
- Analyze findings 
- Create playback report
- Roadmapping exercise with the department 
- Initial feedback from department 
- Present internally for feedback 

Week 4: Iterate and present externally 
- Iterate deck based on feedback 
- Discuss final report with the partner 
- Retro the process

After Exploration: Transition 
- Hand off information internally
- If applicable: offer continued support 

Once everything is said and done, we retro the whole process so we can determine what should be improved for the next time. While this is all happening, our User Researcher is also looking at how the process is working, observing interviews, and running feedback sessions with participants to learn how it went for them. Both of these things inform the next tasks in our backlog. 

## How we iterated upon Explorations

### Research 
Between each pilot, we spent 1 - 2 sprints iterating upon the service based on research findings and team feedback. The goals of these efforts included improving process by iterating on the interview guides and improving the artifacts related to delivering an Exploration. We also used the work to identify and address gaps in the framework.

The work consisted of two research activities, observational study and follow-up discussions, and one team ritual, retros.
- Observational study: The design researcher on the team observed all interviews with the partner as well as team meetings and rituals that took place as a part of the Exploration. The researcher took notes and collated observations from these interviews and team rituals for analysis.
- Follow-up interviews: The design researcher conducted follow-up discussions with people from the partner organisation after the primary Exploration interviews took place. Notes from those discussions were aggregated and analyzed.
- Retros with the team: The project manager held retros after each Exploration to learn from the team what went well and what needed work from each person’s perspective.

Between each Exploration, the researcher presented findings from these efforts back to the team. The project manager drafted action items based on the research findings and retro and added them to the backlog. The project manager then led a team conversation to prioritize tasks for the next sprint and Exploration. During the iteration sprints, we followed a regular Agile approach with some needed adjustments due to the nature of the work. 

### Team model 

Early in the project, we thought that each Exploration would be run by a different team each time, recruited from a rotating pool of interested parties within CDS. We learned that model wouldn't work for a number of reasons: 
- Capacity challenges meant that we didn't have a guarantee there would always be people available when we need to run an Exploration, or we ran the risk the people might have competing priorities. This created a lot of instability and uncertainty, sometimes right up until the last minute before the sprint. 
- Not having continuity on the team meant a worse service for our partners - it was important to have at least a few members on the team who had done previous Explorations, because they were able to bring lessons learned forward about how the Sprints were run, as well as insights about what we're seeing in common across multiple departments.

After our first pilot, we tried out having a single team that would both make improvements to the service, as well as deliver it. We would conduct the 4-week Exploration Sprint, and then iterate over 1 - 2 sprints before running the next Exploration. During the window of iteration, our PM would lead the team, and during the Exploration Sprint our Sprint Lead would. For more information on what the skill make-up of our team looked like, see the "Stand up a dedicated Exploration sprint team section" of our Sprint Guide. 

Moving forward, the Exploration Service is going to be moved within our Consulting Service, so the team model will likely continue to change as we learn about how to best deliver Explorations.

## Learnings to date

### New enabling conditions
- Literature in dev ops and our pilot research findings surfaced the importance of an organizational culture that is open to change and adopting new practices
- In designing timelines with departments, we found that funding limitations and hiring practices can affect feasibility and slow down timelines, so they need to be taken into account early on  

### Value of Explorations

Though we don't have a lot of outcome data yet, follow-up interviews and observations revealed how the process itself is of value: 
- Participants learn from listening to one another 
- Partners using the Exploration to inform their roadmap 

We can contunue making the Explorations more efficient and design for providing this impact.

### Explorations inform Partnerships and Partnerships inform Explorations

When we conduct an Exploration, the whole team functions as researchers listening to government. Consequently, we’ve learned more about their needs which means we can share this with our Partnerships business unit. And as we design more flexible and varied partnership options, we can better set expectations during the Exploration about what's possible afterwards. 

### Varied government landscape 

We don’t have enough data yet to see trends, but we have seen two very different examples that show the contrast in deparmental conditions.
