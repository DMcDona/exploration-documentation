# How we created – and iterated on – the Exploration service and framework

For a detailed overview of how an Exploration phase works, see our [Exploration Sprint Guide](https://github.com/cds-snc/exploration-documentation/blob/main/ExplorationSprintGuide.md).

## Table of contents 
- Why CDS does Explorations 
- Developing the service 
- Naming the service
- What is an Exploration? 
- How we iterated on Explorations
- Our learnings so far

## Why we do Explorations

### The problem: Partnership challenges 

One of the ways CDS works with government organizations is through [Partnerships](https://digital.canada.ca/partnerships/). In the 3 years that CDS has existed, our partnerships have often been difficult and slow to find lasting success. 

Our delivery teams expected to roll up their sleeves and get to work quickly, but encountered unforseen blockers. Constraints such as lengthy and slow approval processes, difficulty getting access to infrastructure like databases, unexpected legal input, unclear problem statements, and simple fear of launching something new, have prevented CDS from building *with* partners and sharing our work publicly. In some cases, these constraints have even prevented services from going live at the final stages.

This created a difficult and stressful working environment for our delivery teams. They were often treated like vendors rather than partners. Our partnerships were seen as having a high potential to fail. Because of these constraints, CDS wasn’t doing a good job delivering on our mandate to change government to serve people better.

What we want is to work with partners who are open and receptive to new ways of working: Where they provide access to the people, information and infrastructure we need, and they’re comfortable with us sharing our work, methodologies and experiences beyond the project space. 

In May of 2020, CDS launched work on a new **Exploration** service, dedicating a multi-disciplinary team to ideate and iterate on what such a service could look like. 

### Goals of an Exploration
Through experimentation and consulting with partners and leadership, the goals of an Exploration are:  
- To explore the partner’s capabilities and readiness to deliver a digital service and provide practical actions to improve.
- To determine CDS’ level of involvement in delivering a new service.
- To understand the government landscape and learn about patterns and trends in potential partner organizations.

## Developing the Explorations service

### 1. Conduct background research

#### Secondary research 
We started by reviewing other frameworks and models for creating change, such as maturity models and the USDS Discovery Sprint model. We thought it was possible to repurpose content from other sources, but quickly realized that a maturity model didn't fit the problem we were trying to solve. Instead, we mapped out common themes we were seeing in those models. The resulting "thematic model" became the foundation for our framework. 

We also adapted the structure of sprints from the [USDS Discovery Sprint](https://sprint.usds.gov/preparing/) model. Note: the USDS documentation linked here is a newer iteration of the one that we worked with at the beginning of our project.   

#### Primary research 
We collected primary research from all of the delivery communities in CDS. We gathered their perspectives on what the necessary conditions for digital service delivery were, based on their experience in government partnerships. We mapped these needs to the thematic model created from secondary research, and together that became a framework for understanding how Explorations could point to making change in government.

![Digital sticky notes organized on a grid with two axes. Down the left axis there's 3 rows labelled "Strategic Direction and Leadership", "Enabling Capacity" and "Enabling Delivery". Along the top axis there's 9 columns labelled "Cross-discipline", "Research", "Design", "Dev/Engineering/Security", "Partnerships", "Outreach", "Policy", "Product", and "Screening".](https://github.com/cds-snc/exploration-documentation/blob/main/images/Assessment%20Criteria%20Synthesis.png)

#### Peer feedback and consultations
In the early stages of the process we met with peers across other organizations including the Ontario Digital Service (ODS), Government Digital Service (GDS), and CodeforCanada(C4C) to gather feedback from them. We also engaged consultations from global organizations like Public Digital, and the Harvard Kennedy School. 

### 2. Develop a theory of change
We created a [theory of change](https://trello.com/c/IXrBSPhx/1-monitoring-and-evaluation-workshop-from-merlin-chatwin-c4c-and-laura-nelson-hamilton-ods) for the service, which informed our evaluation & monitoring plan. 

![Diagram made up of rectangles and arrows. The main part of the diagram starts with two boxes labelled "Activities" and "Outputs", an arrow points to the next box labelled "Reach and reaction". The boxes that follow (directed by arrows) are "Capacity changes", "Behaviour changes", "Immediate benefits", "Long-term changes". A vertical rectangle down the left side of the diagram has descriptions of what each of these things mean. On the right side of the diagram there are additional boxes that list out "Assumptions", "Supporting activities", "Possible indicators", and possible data sources for each of the afformentioned boxes.](https://github.com/cds-snc/exploration-documentation/blob/main/images/Pre-Discovery%20Theory%20of%20Change.png)

### 3. Prototype the framework and the service

We started by prototyping the framework criteria and questions. At the same time, we also mapped the Exploration Service Model and developed processes and artifacts to support delivery of the service. Deliverables in this stage included draft interview guides, maps of what different levels of CDS engagement with partners could look like, and stakeholder mapping tools. 

### 4. Create a CDS intake model 
We identified a need for a more defined and centralized intake process, so we also took a broader look at CDS as a whole and mapped out all of the organization's service lines/lines of business. We then created a centralized intake process to serve departments coming into our pipeline.

### 5. Service pilots and iterations

Since September of 2020, we have piloted with 4 programs, in 3 departments:
- ESDC Temporary Foreign Worker Program 
- NRCan CanmetENERGY LEEP Program
- NRCan CCMEO Flood Mapping Program
- IRCC Refugee Information Needs Initiative

Throughout the pilot engagements, we collect research insights on what within the process is or isn't working, and then we spend 1-2 sprints iterating on the Exploration process before starting another pilot.

## Naming the service

The service has changed as constraints and goals became more defined, which is reflected in the number of times we've iterated on the name: 

**1. Departmental Assessment** 

The service began as a "vetting" tool, a "go/no go" on whether we would partner with a department. But we found that this name only described the part of the service that was evaluative. It sounded like an exam, and introduced a power dynamic between the organization doing the assessing (CDS) and the one being assessed (our partners). 

**2. Pre-Discovery** 

We moved to the term "Pre-Discovery" because it sounded less intimidating and it fit into our "Discovery, Alpha, Beta, Live" model of phase patterns – which helped communicate the process. After running several pilot engagements with partners, it became evident that the end might not always lead to a Discovery phase. We learned there's multiple ways of helping a department, so by calling the service "Pre-Discovery" we weren't managing expectations effectively. We also found that the process had intrinsic value for a partner, and it could exist as its own service. We decided to separate the name from the "Discovery, Alpha, Beta, Live" model.

**3. Exploration** 

The latest iteration is called an "Exploration". We chose this name because it is more reflective of the work actually being done. We are exploring digital service conditions by adapting to a new environment, and providing specific recommendations and guidance. In other words, we're meeting partners where **they're** at. Failure is okay when exploring, and can bring value and insights as we document and chart a path to build a larger view of the service landscape for partners. Even if an Exploration doesn’t lead to a Discovery, there are many small discoveries made along the way. 

## What is an Exploration? 

We have several hypotheses: 
1. If CDS better understands the existing conditions within a service context, we can help a department more effectively, and
2. If enabling conditions and practices are present, teams will deliver better products and services. 

Departments often approach CDS for help, but it's hard to know how we might best support them without having a better picture of the service opportunity and the delivery conditions and practices in place. By conducting an Exploration, we help the department take stock, while building our own understanding to inform potential next steps. 

Through discussions with executives, team members, and departmental representatives across a variety of functional areas (i.e. policy, IT, program delivery, communications, legal, and privacy), CDS looks at the existing conditions and practices and the extent to which they enable digital service delivery.  

This structured approach helps CDS deliver timely, incremental value to the department – and gives us the information we need to inform if and how CDS might continue to partner with them. This could be through consulting, providing an embedded service design and delivery team, or by connecting departments to CDS platform services.

An Exploration is comprised of a **framework** we created and a **service** built on top of it. The framework includes criteria and assessment tools. The service is the 4-week process and all of the supporting artifacts that we use to deliver value to the department. We separate the framework and the service into two parts because the framework could also potentially be used as a foundation for other services, such as a self-assessment service.

As we deliver the service, we’re learning and iterating on both, to improve the Exploration experience and the value that comes from participating in and running one.

### Framework: exploring enabling conditions

We use the following two criteria groups to inform recommendations for digital delivery within a particular service context and opportunity:

*To what extent do digital roles and structures exist?*
- Leadership and coherent direction (coherent policy direction, service priorities and champions)
- Enabling capacity and infrastructure (modern IT infrastructure, legal, comms and privacy guidance and support)
- Delivery capability (talent and teams with digital skills, tactics and tools)
 
*To what extent do digital practices exist?* 
- Establishing and empowering multidisciplinary teams; 
- Conducting user research to make service design decisions informed by user feedback;
- Practicing continuous improvement; 
- Working in the open; 
- Cultivating a culture of collaboration; and, 
- Shifting to a service delivery model (i.e., ensuring prioritization and funding for digital services, including talent development and recruitment and access to modern tools and infrastructure).

### Service: where Explorations live in the CDS service model 

This is a map of CDS' service model. To read the boxes more closely, click on the image. 

![Diagram with rectangles. On the left side there's a rectangle labelled "Outreach", which points to "Intake", which then points to 4 different boxes. From top to bottom the boxes are labelled "Partnerships:Embedded Service Design and Delivery", "Partnerships:Exploration", Partnerships: Consulting", "Platform: Notify", "Platform:Forms". Within each of the boxes is there are more boxes showing a process through directional arrows.](https://github.com/cds-snc/exploration-documentation/blob/main/images/Business%20units%20service%20map.png)

Each of the squares represents one of the services that we offer – such as Notify, Forms, Embedded Partnerships, etc – with arrows showing how they complement each other and work together. 

Most services start with the new Intake process on the left side of the map. This is how departments connect with CDS, and get triaged to different units depending on their needs. Intake may lead to an Exploration (the second box down on the right side), which in turn may lead to other services within CDS. 

### Service: How the process works

If you want an in-depth look at the process, check out our [How to set up and run an Exploration](https://github.com/cds-snc/exploration-documentation/blob/main/ExplorationSprintGuide.md) guide. 

This is the map of the Exploration service. To read the boxes more closely, click on the image. 

![A grid. Down the left axis there are two sections labelled "Front-stage" and "Back-stage". In the first section, the rows are labelled "Stage", "Phase", "Department actors", "CDS actors", and "Touchpoints". In the second section, the rows are labelled "Department actors", "CDS actors", "Supporting artifacts", "Outputs", and "Pains". Along the top axis, in the "Stage" row, the headings are as follows: "Outreach", "Intake", "Before Exploration", "Exploration", and "Post Exploration". In the second row called "Phase", the boxes are as follows: "Unawareness", "Awareness", "Entry", "Initial Qualify (if possible)", "Introduction/final qualify", "Generating support", "Set-up", "Phase 1: Onboarding", "Phase 2: Gather information", "Phase 3: Synthesize and present internally", "Phase 4: Iterate and collaborate externally", "Partnership design", and "Partnership set-up". In each of the columns there is supporting text that corresponds which each row.](https://github.com/cds-snc/exploration-documentation/blob/main/images/Exploration_service_map.png)

Explorations are designed to take approximately 4 weeks, with a set-up phase prior to that.

Before Exploration: Set-up 
- Describe the problem space
- Map key stakeholders
- Gather background documents
- Meet with the partner to set up the sprint

Week 1: Internal onboarding 
- CDS team onboarding meeting
- Read partner documentation 
- Review and tailor sprint materials to the partner

Week 2: Gather information 
- Kick-off meeting 
- Interviews
- Post-interview debrief sessions
- Scoring the partner according to the Explorations framework criteria

Week 3: Synthesize and share internally 
- Follow-up discussions with department
- Analyze findings 
- Create a "playback report"
- Share the report internally for feedback 

Week 4: Iterate and share with the partner
- Roadmapping exercise with the partner 
- Initial feedback from department 
- Iterate based on feedback 
- Discuss final report with the partner 
- Perform a retrospective on the process

After Exploration: Transition 
- Hand off information internally
- If applicable: offer continued support to the partner

## How we iterated on Explorations

### Research 
Between each pilot, we spent 1–2 sprints iterating on the Exploration service based on research findings and team feedback. The goals included improving the overall process, as well as refining artifacts related to delivering an Exploration – such as our interview guides or our sprint guidance documents. We also identified and addressed gaps in the framework, by refining our categories and developing new questions.

To gather insights, we conducted two research activities: observational study and follow-up discussions and retrospectives.
- Observational study: The design researcher on the team observed all interviews with the partner as well as team meetings and rituals that took place as a part of the Exploration. The researcher took notes and collated observations from these interviews and team rituals for analysis.
- Follow-up interviews: The design researcher conducted follow-up discussions with people from the partner organisation after the primary Exploration interviews took place. Notes from those discussions were aggregated and analyzed.
- Retros with the team: The project manager held retros after each Exploration to learn from the team what went well and what needed work from each person’s perspective.

Between each Exploration, the researcher presented findings from these efforts back to the team. The project manager drafted action items based on the research findings and retro and added them to the backlog. The project manager then led a team conversation to prioritize tasks for the next sprint and Exploration. During the iteration sprints we followed a regular Agile approach, though we adjusted it to meet the needs of the project.

## Our learnings so far

### New enabling conditions
- Secondary research and and our experience with pilot sprints surfaced the importance of an organizational culture that is open to change and adopting new practices.
- When designing timelines with departments, we found that funding limitations and hiring practices can affect feasibility and slow down timelines – so they need to be taken into account early on.

### Value of Explorations

Though we don't have a lot of outcome data yet, follow-up interviews and observations revealed how the process itself is of value: 
- Participants learn from listening to one another.
- Partners using the Exploration to inform their roadmap.

We can continue making Explorations more efficient and design for providing this impact.

### Explorations and Partnerships inform each other

When we conduct an Exploration, the whole team functions as researchers listening to government. We’ve learned more about departmental needs, which means we can share this with our Partnerships business unit. As we design more flexible and varied partnership options, we can better set expectations during the Exploration about what's possible afterwards. 

### Team model 

Early in the process, we thought that each Exploration would be run by a different team, recruited from a rotating pool of interested parties within CDS. We learned that model wouldn't work for a number of reasons: 
- Capacity challenges meant that we didn't have a guarantee there would always be people available when we need to run an Exploration, or we ran the risk the people might have competing priorities. This created a lot of instability and uncertainty, sometimes right up until the last minute before the sprint. 
- Not having continuity on the team meant a worse service for our partners - it was important to have at least a few members on the team who had done previous Explorations, because they were able to bring lessons learned forward about how the Sprints were run, as well as insights about what we're seeing in common across multiple departments.

After our first pilot, we tried out having a single team that would both make improvements to the service, as well as deliver it. We would conduct the 4-week Exploration Sprint, and then iterate over 1 - 2 sprints before running the next Exploration. During the window of iteration, our PM would lead the team, and during the Exploration Sprint our Sprint Lead would. For more information on what the skill make-up of our team looked like, see the "Stand up a dedicated Exploration sprint team section" of our [Sprint Guide](https://github.com/cds-snc/exploration-documentation/blob/main/ExplorationSprintGuide.md). 

Moving forward, the Exploration Service is going to be moved within our Consulting Service, so the team model will likely continue to change as we learn about how to best deliver Explorations.

### Varied government landscape 

We don’t have enough data yet to see trends, but we have seen two very different examples that show the contrast in departmental conditions.

