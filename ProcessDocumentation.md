# How we created and iterated on Explorations 

## Table of contents 
- Why Exploration? 
- What is an Exploration? 
- How we developed it 

## Why Exploration

### The problem: Partnership challenges 

One of the ways that CDS works with departments is through **Partnerships** where we work on a problem space together. But in the 3 years that CDS has existed, our Partnerships have been hard and slow.

Our delivery teams expected to roll up their sleeve and get to work from the get go, but instead ran into blockers. Constraints, such as lengthy and slow approvals, difficulty in getting access to key infrastructure, unexpected legal opinions, fuzzy problem statements and fear of launching something new, have prevented us from building with partners and sharing our work publicly. In some cases it’s even prevented us from taking the final step to go live. 

This lead to a difficult and stressful working environment for delivery teams. They were often treated like vendors rather than partners. Our partnerships had a high potential to fail. And because of that, we weren’t doing a good job delivering on our mandate to change government to serve people better.

What we want is to work with partners who are open and receptive to new ways of working: Where they provide access to the people, information and infrastructure we need. And they’re comfortable with us sharing our work, methodologies and experiences beyond the project space.  

So in May of 2020, we launched our work on our **Exploration Service**.

#### Goals of an Exploration
It's been a long journey to get here, and our goals have changed along the way, but here is where they stand now:  
- **Goal 1:** Explore the partner’s capabilities and readiness to deliver a digital service and provide practical actions to improve.
- **Goal 2:** Determine CDS’ level of involvement.
- **Goal 3:** Understand the government landscape and learn about patterns and trends.

## How we developed the service

**1. Conducted background research** 
- Primary research: collected needs from all delivery communities about what makes a good partnership
- Secondary research: other frameworks (maturity models) 
- Peer feedback: ODS, GDS & C4C
- Global lens: Public Digital & Harvard Kennedy School consultations 

**2. Developed a Theory of Change**
- Created a Theory of Change for the service
- This informed an evaluation & monitoring plan 

**3. Prototyped the framework and the service**
- Built the Framework: Assessment criteria & questions 
- Developed processes & artifacts to support the service

**4. Created an org Intake model** 
- Centralized Intake for how departments come into our pipeline
- Mapped & connected CDS business units 

**5. Piloted with 4 departments, iterating between each round**
- Temporary Foreign Worker Program 
- NRCan CanmetENERGY
- NRCan Flood mapping 
- IRCC Refugee resettlement

## Naming the service: what's in a name? 

The service has evolved immensely, which is reflected in the number of times we've iterated on the name: 

**1. Departmental Assessment** 

The service began as a "vetting" tool, a "go/no go" on whether we would partner with a department. But we found that this named only described the part of the service that was evaluative. It sounded a bit like an exam and introduced a power dynamic between the org doing the assessing and the one being assessed. 

**2. Pre-Discovery** 

We moved to the term "Pre-Discovery", to make it sound less intimidating, and also because it fit into our "Discovery, Alpha, Beta, Live" model of phase patterns, which helped communicate what the process was. But after running a couple it became evident that it might not always lead to a Discovery phase, that there's multiple ways of helping a department. By calling it a "Pre-Discovery" we weren't managing expectations effectively. We also found that the process had value in and of itself for a partner, and it could exist as its own service. So we decided to separate the name from the "Discovery, Alpha, Beta, Live" model.

**3. Exploration** 

The latest iteration, and where we've landed currently is "Exploration". We chose this name because we thought it was more reflective of what we're actually doing. We are Exploring digital service conditions by adapting to a new environment: meaning meeting new partners where **they** are at. Failures are okay when exploring and can bring high value and insights as we document and chart a path to build a larger view of the landscape. And even if it doesn’t lead to a Discovery, there are many useful small discoveries along the way. 

## What is an Exploration? 

We have a couple of hypotheses: 
1. If CDS better understands the existing conditions within a service context then it will be able to better tailor its help to a department, and
2. If enabling conditions and practices are present then teams will deliver better products and services. 

An Exploration is a service we deliver to:
- Understand the existing delivery conditions and practices within a service context;
- Identify ways to introduce digital practices, like conducting user research and practicing continuous improvement;
- Make recommendations on ways to shift conditions to enable those practices, if and where needed;
- Provide advice and practical actions to develop and deliver a product or service – or improve an existing one; and
- Explore how CDS might support a partner on that journey.

An Exploration is comprised of a framework and the service, and as we deliver them we’re learning and iterating on both to improve the Exploration experience and the value that comes from participating in one.

### Framework: Exploring enabling conditions

To learn about a department's conditions we try to answer these two questions:

*To what extent do digital roles and structures exist?*
- Leadership and coherent direction (coherent policy direction, service priorities and champions)
- Enabling capacity and infrastructure (modern IT infrastructure, legal, comms and privacy guidance and support)
- Delivery capability (talent and teams with digital skills, tactics and tools)
 
*To what extent do digital practices exist?* 
- Shifting to a product delivery model
- Cultivating a culture of collaboration 
- Working in the open 
- Establishing and empowering multi-disciplinary teams 
- Conducting user research 
- Practicing continuous improvement

### Service: Where Explorations live in the CDS service model 

As part of our work on this service, we did some mapping of the broader CDS service model, and created a centralized Intake process. 

(add map)

Each of the squares represents one of the services that we offer, such as Notify, Forms, Embedded Partnerships etc and there’s arrows going between all of them, showing how they connect with each other and work together. 

There’s an intake process at the beginning, on the left side. This is how departments come into the organization, and get triaged to different units depending on their needs. 

### Service: How the process works 

If you want an in-depth look at the process, check out our (add link) How to set up and run an Exploration guide. 

For a shorter overview, you can see how we ran the Exploration with the NRCan Flood mapping team. 

(add zoom map)

First, in the fall of 2020, someone from NRCan Flood mapping contacted CDS. Our Head of Business Development qualified whether the request was in our mandate - is it a government org, is the work public-facing? Etc. and is it something that we think is valuable to take on? Once qualified, he had some initial conversations with them to get a sense of the problem space and what ways we may be able to help them. 

It was decided it would probably make the most sense to send them through the Partnerships business line, so we screened the organization to determine whether they were a good candidate for an Exploration. They met the pre-screening criteria, so their partnership was passed into the set up stage for the Exploration process. 

(add process documentation?) 

In the set up stage we worked with the Flood mapping team to gather initial info and schedule meetings with interview participants. Then we started the Exploration, which is designed to take 4 weeks. 

During the first week, the CDS sprint team onboarded to the departmental context. Then in the second week, at the beginning we had a kick-off meeting with the Exploration team and the NRCan stakeholders. After the kick-off, we ran a series of interviews with stakeholders across the different functional groups. Each day we would debrief the interviews internally and score the criteria. In the third week we document our insights and create a playback report of our findings and recommendations. We discuss the report internally with senior management, who provides us valuable feedback on the narrative, the recommendations, and the proposed path forward. In the fourth week, we iterate on the report based on the feedback, (change this process) send it to the department and then have a meeting to discuss the content and next steps.

Once everything is said and done, we retro the whole process so we can determine what should be improved for the next time. While this is all happening, our User Researcher is also looking at how the process is working, observing interviews, and running feedback sessions with participants to learn how it went for them. 

