# Process documentation: The "Why, How and What" of Exploration 

## Table of contents 
- Why Exploration? 
- How we developed it 
- What is an Exploration? 

## Why Exploration

### The problem: Partnership challenges 

One of the ways that CDS works with departments is through **Partnerships** where we work on a problem space together. But in the 3 years that CDS has existed, our Partnerships have been hard and slow.

Our delivery teams expected to roll up their sleeve and get to work from the get go, but instead ran into blockers. Constraints, such as lengthy and slow approvals, difficulty in getting access to key infrastructure, unexpected legal opinions, fuzzy problem statements and fear of launching something new, have prevented us from building with partners and sharing our work publicly. In some cases it’s even prevented us from taking the final step to go live. 

This lead to a difficult and stressful working environment for delivery teams. They were often treated like vendors rather than partners. Our partnerships had a high potential to fail. And because of that, we weren’t doing a good job delivering on our mandate to change government to serve people better.

What we want is to work with partners who are open and receptive to new ways of working: Where they provide access to the people, information and infrastructure we need. And they’re comfortable with us sharing our work, methodologies and experiences beyond the project space.  

So in May of 2020, we launched our work on our **Exploration Service**.

#### Goals of an Exploration
It's been a long journey to get here, and our goals have changed along the way, but here is where they stand now:  
- **Goal 1:** Explore the partner’s capabilities and readiness to deliver a digital service and provide practical actions to improve.
- **Goal 2:** Determine CDS’ level of involvement.
- **Goal 3:** Understand the government landscape and learn about patterns and trends.

## How we developed the service

**1. Conducted background research** 
- Primary research: collected needs from all delivery communities about what makes a good partnership
- Secondary research: other frameworks (maturity models) 
- Peer feedback: ODS, GDS & C4C
- Global lens: Public Digital & Harvard Kennedy School consultations 

**2. Developed a Theory of Change**
- Created a Theory of Change for the service
- This informed an evaluation & monitoring plan 

**3. Prototyped the framework and the service**
- Built the Framework: Assessment criteria & questions 
- Developed processes & artifacts to support the service

**4. Created an org Intake model** 
- Centralized Intake for how departments come into our pipeline
- Mapped & connected CDS business units 

**5. Piloted with 4 departments, iterating between each round**
- Temporary Foreign Worker Program 
- NRCan CanmetENERGY
- NRCan Flood mapping 
- IRCC Refugee resettlement

## Naming the service: what's in a name? 

The service has evolved immensely, which is reflected in the number of times we've iterated on the name: 

**1. Departmental Assessment** 

The service began as a "vetting" tool, a "go/no go" on whether we would partner with a department. But we found that this named only described the part of the service that was evaluative. It sounded a bit like an exam and introduced a power dynamic between the org doing the assessing and the one being assessed. 

**2. Pre-Discovery** 

We moved to the term "Pre-Discovery", to make it sound less intimidating, and also because it fit into our "Discovery, Alpha, Beta, Live" model of phase patterns, which helped communicate what the process was. But after running a couple it became evident that it might not always lead to a Discovery phase, that there's multiple ways of helping a department. By calling it a "Pre-Discovery" we weren't managing expectations effectively. We also found that the process had value in and of itself for a partner, and it could exist as its own service. So we decided to separate the name from the "Discovery, Alpha, Beta, Live" model.

**3. Exploration** 

The latest iteration, and where we've landed currently is "Exploration". We chose this name because we thought it was more reflective of what we're actually doing. We are Exploring digital service conditions by adapting to a new environment: meaning meeting new partners where **they** are at. Failures are okay when exploring and can bring high value and insights as we document and chart a path to build a larger view of the landscape. And even if it doesn’t lead to a Discovery, there are many useful small discoveries along the way. 

## What is an Exploration? 

We have a couple of hypotheses: 
1. If CDS better understands the existing conditions within a service context then it will be able to better tailor its help to a department, and
2. If enabling conditions and practices are present then teams will deliver better products and services. 

An Exploration is a service we deliver to:
- Understand the existing delivery conditions and practices within a service context;
- Identify ways to introduce digital practices, like conducting user research and practicing continuous improvement;
- Make recommendations on ways to shift conditions to enable those practices, if and where needed;
- Provide advice and practical actions to develop and deliver a product or service – or improve an existing one; and
- Explore how CDS might support a partner on that journey.

An Exploration is comprised of a framework and the service, and as we deliver them we’re learning and iterating on both to improve the Exploration experience and the value that comes from participating in one.

### Framework: Exploring enabling conditions

To learn about a department's conditions we try to answer these two questions:

*To what extent do digital roles and structures exist?*
- Leadership and coherent direction (coherent policy direction, service priorities and champions)
- Enabling capacity and infrastructure (modern IT infrastructure, legal, comms and privacy guidance and support)
- Delivery capability (talent and teams with digital skills, tactics and tools)
 
*To what extent do digital practices exist?* 
- Shifting to a product delivery model
- Cultivating a culture of collaboration 
- Working in the open 
- Establishing and empowering multi-disciplinary teams 
- Conducting user research 
- Practicing continuous improvement

### Service: Where Explorations live in the CDS service model 

As part of our work on this service, we did some mapping of the broader CDS service model, and created a centralized Intake process. 

(add map)

Each of the squares represents one of the services that we offer, such as Notify, Forms, Embedded Partnerships etc and there’s arrows going between all of them, showing how they connect with each other and work together. 

There’s an intake process at the beginning, on the left side. This is how departments come into the organization, and get triaged to different units depending on their needs. 

### Service: How the process works (overview)

If you want an in-depth look at the process, check out our (add link) How to set up and run an Exploration guide. 

Explorations are designed to take approximately 4 weeks, with a set up phase prior to that. The process goes as follows... 

Before Exploration: Set-up 
- Describe the problem space
- Map key stakeholders
- Gather background documents

Week 1: Internal onboarding 
- CDS team onboarding meeting
- Read partner documentation 
- Review and tailor sprint materials 

Week 2: Gather information 
- Kick off meeting 
- Interviews
- Debriefs
- Scoring

Week 3: Synthesize and present internally 
- Follow up discussions with department
- Analyze findings 
- Create playback report
- Roadmapping exercise with the department 
- Initial feedback from department 
- Present internally for feedback 

Week 4: Iterate and present externally 
- Iterate deck based on feedback 
- Present final report to partner 
- Retro the process

After Exploration: Transition 
- Hand off information internally
- If applicable: offer continued support 

Once everything is said and done, we retro the whole process so we can determine what should be improved for the next time. While this is all happening, our User Researcher is also looking at how the process is working, observing interviews, and running feedback sessions with participants to learn how it went for them. Both of these things inform the next tasks in our backlog. 

